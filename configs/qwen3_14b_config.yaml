# Qwen3-14B Training Configuration for AWS Trn2

# Model Configuration
model:
  name: "unsloth/Qwen3-14B"
  vocab_size: 152064
  hidden_size: 5120
  num_layers: 40
  num_attention_heads: 40
  num_key_value_heads: 8
  max_position_embeddings: 32768
  intermediate_size: 20480  # 4 * hidden_size
  rms_norm_eps: 1e-6
  rope_theta: 10000.0
  hidden_act: "silu"
  gradient_checkpointing: true
  use_cache: false  # Disabled during training

# Training Configuration
training:
  # Basic training parameters
  batch_size: 4  # Per device batch size
  gradient_accumulation_steps: 8  # Effective batch size = batch_size * gradient_accumulation_steps * world_size
  learning_rate: 1e-5
  num_epochs: 3
  max_seq_length: 2048  # Start with shorter sequences for memory efficiency
  
  # Optimization settings
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Learning rate schedule
  warmup_ratio: 0.1
  lr_schedule_type: "linear"  # linear, cosine, polynomial
  
  # Mixed precision training
  bf16: true  # Use BF16 for Trn2
  fp16: false
  
  # Logging and checkpointing
  logging_steps: 50
  save_steps: 500
  evaluation_steps: 500
  
  # Compilation settings
  compile_model: true
  
  # Advanced settings
  num_workers: 4
  pin_memory: true
  dataloader_drop_last: true

# Data Configuration
data:
  train_path: "data/train.jsonl"
  validation_path: "data/validation.jsonl"
  dataset_type: "text"  # "text" or "conversation"
  text_column: "text"
  conversation_format: "chatml"  # For conversation datasets
  preprocessing:
    remove_empty: true
    min_length: 10
    max_length: 4096

# Distributed Training
distributed:
  enabled: true
  backend: "xla"  # Use XLA backend for Neuron
  find_unused_parameters: false

# AWS Neuron Specific Settings
neuron:
  compiler_workdir: "./neuron_cache"
  compiler_flags: "--model-type=transformer"
  optimization_level: "2"
  enable_debug: false
  
  # Memory optimization
  enable_mixed_precision: true
  enable_stochastic_rounding: true
  
  # Performance tuning
  tensor_parallel_size: 1  # Increase for multi-chip training
  pipeline_parallel_size: 1
  
  # Monitoring
  enable_profiling: false
  profile_steps: 100

# Hardware Configuration
hardware:
  instance_type: "trn2.48xlarge"  # Adjust based on your instance
  num_devices: 6  # Number of Trainium chips
  device_memory_gb: 128  # Memory per device
  
# Output and Logging
output_dir: "./output"
log_dir: "./logs"
log_level: "INFO"

# Checkpointing
checkpointing:
  save_best_model: true
  save_total_limit: 3
  resume_from_checkpoint: null  # Path to checkpoint to resume from

# Monitoring and Evaluation
monitoring:
  use_tensorboard: true
  use_wandb: false  # Set to true if using Weights & Biases
  wandb_project: "qwen3-14b-trn2"
  
  # Metrics to track
  track_memory_usage: true
  track_throughput: true
  track_loss_components: true

# Early Stopping
early_stopping:
  enabled: false
  patience: 3
  min_delta: 0.001
  metric: "eval_loss"
  mode: "min"

# Model Saving
model_saving:
  save_optimizer_state: true
  save_scheduler_state: true
  save_tokenizer: true
  save_model_config: true
  
  # HuggingFace Hub integration
  push_to_hub: false
  hub_model_id: null
  hub_strategy: "every_save"